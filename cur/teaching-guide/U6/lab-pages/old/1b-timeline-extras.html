<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<script src="/bjc-r-ita/llab/loader.js" type="text/javascript"></script>
<title>Unit 6 Lab 1: A Timeline of Computers, Page 2</title>
<style type="text/css">
      most-important {
        color: #f00;
      }
    </style>
</head>
<body>
<h2>A Brief History of Computers</h2>
<h3>Timeline Extras</h3>

<div class="timeline collapse" id="timeline-hollerith">
<div class="endnote">
<h4><strong>First Punched Card Tabulator</strong> (c. 1890)</h4>
<p>
          Before the programmable digital computer, the first "big data"
          application was the US Census. Herman Hollerith developed a system of
          storing information encoded by punching holes in cardboard cards.
          These cards were processed by non-programmable "tabulating equipment,"
          such as a card sorter, that could take a deck of cards and arrange
          them in numeric order based on a subset of the columns. Hollerith's
          technology was bought by IBM and was the beginning of their
          involvement in computation.
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-analog">
<div class="endnote">
<h4><strong>Analog Computers</strong> (c. 1900)</h4>
<div class="sidenoteBig">
<img alt="analog computer" src="/bjc-r-ita/img/6-computers/analog.png"/><br/><small><small>analog computer<br/>From http://chalkdustmagazine.com/</small></small>
</div>
<p>
          In analog computers, a wire wasn't either fully on or fully off, but
          could carry any voltage (continuously variable) in the machine's
          operating range. Because all voltages were legal, these machines
          suffered from electronic noise and so their precision was limited,
          typically to two or three decimal digits. But for a certain class of
          problems—in particular, solving differential equations—these machines
          were a natural fit. (Babbage's Difference Engine solved a very similar
          class of problems, but with finite differences instead of continuous
          differentials.)
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-enigma">
<div class="endnote">
<h4><strong>The Enigma</strong> (1930s)</h4>
<div class="sidenoteBig">
<img alt="Enigma machine model 1" src="/bjc-r-ita/img/6-computers/enigma.jpg"/><br/><small><small>Enigma model 1<br/>By Alessandro Nassiri - Museo della Scienza e
              della Tecnologia "Leonardo da Vinci", CC BY-SA 4.0</small></small>
</div>
<p>
          The <strong>Enigma machines</strong> were a series of
          electro-mechanical rotor cipher machines developed and used in the
          early- to mid-20th century to protect commercial, diplomatic and
          military communication. Enigma was invented by the German engineer
          Arthur Scherbius at the end of World War I. Early models were used
          commercially from the early 1920s, and adopted by military and
          government services of several countries, most notably Nazi Germany
          before and during World War II. Several different Enigma models were
          produced, but the German military models, having a plugboard, were the
          most complex. Japanese and Italian models were also in use.
        </p>
<p>
          Though Enigma had some cryptographic weaknesses, in practice it was
          German procedural flaws, operator mistakes, failure to systematically
          introduce changes in encipherment procedures, and Allied capture of
          key tables and hardware that, during the war, enabled Allied
          cryptologists to succeed and "turned the tide" in the Allies' favor.
        </p>
<p>
<small><small>from Wikipedia</small></small>
</p>
</div>
</div>
<div class="timeline collapse" id="timeline-church">
<div class="endnote">
<h4><strong>Lambda Calculus</strong> (1936)</h4>
<div class="sidenoteBig">
<img alt="Alonzo Church" height="251" src="/bjc-r-ita/img/6-computers/church.png" width="194"/>
<br/><small><small>Alonzo Church</small></small>
</div>
<p>
          Two people created a theoretical framework for computer science in the
          same year; Alonzo Church beat Alan Turing by just a few months. But
          there is a crucial difference between their two approaches. Turing's
          better-known idea, the <em>Turing machine,</em> represents a
          computation as a series of steps, with provision for conditionals and
          looping. Since the purpose of this theoretical machine is to prove
          theorems about computing, not to do actual computations, its data
          representation is limited to a "tape," infinite in both
          directions, with cells that can contain one of a small number of
          symbols. The symbols are sometimes represented as 0 and 1, but this is
          slightly misleading because numbers are not represented in binary as
          on practical computers. Instead they use a conceptually simpler system
          called "unary," although it's not a positional system like
          binary. The number five, for example, could be represented as 0111110,
          with five ones, delimited by a zero at both ends. So, representing the
          number 54321 would take 54,323 cells on the tape, all ones except for
          the two zeros at the end.
        </p>
<p>
          The Turing machine is a theoretical description of
          <em>imperative</em> programming, with a program as a sequence of
          commands that operate by changing values in memory. This is why the AP
          CSP writers want you to learn that the fundamental building blocks of
          programs are <em>sequence, conditionals, and looping.</em>
</p>
<p>
          By contrast, Church's theoretical framework is based on
          <em>functions, </em>The basic operations in a program are procedure
          creation, like the grey rings in Snap<em>!</em>, and procedure
          calling, like the <code>run</code> block in Snap<em>!</em>. That turns
          out to be all you need for a complete programming language. All the
          rest—conditionals, looping, arithmetic, lists, Booleans, the works—can
          be created from those two operations.
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-turing-machine">
<div class="endnote">
<h4><strong>The Turing Machine</strong> (1936)</h4>
<p></p>
</div>
</div>
<div class="timeline collapse" id="timeline-lisp">
<div class="endnote">
<h4><strong>LISP</strong> (1958)</h4>
<p></p>
</div>
</div>
<div class="timeline collapse" id="timeline-tcp-ip">
<div class="endnote">
<h4><strong>TCP/IP</strong> (1970s)</h4>
<p></p>
</div>
</div>
<div class="timeline collapse" id="timeline-alto">
<div class="endnote">
<h4><strong>The Xerox Alto</strong> (1973)</h4>
<p></p>
</div>
</div>
<div class="timeline collapse" id="timeline-first-pc">
<div class="endnote">
<h4><strong>The Altair 8800</strong> (1974)</h4>
<div class="sidenoteBig">
<img alt="Altair 8800 computer" height="25%" src="/bjc-r-ita/img/6-computers/altair.jpg"/><br/>
<small><small>
              Altair 8800 at the Computer History Museum<br/>
              Todd Dailey, 2009, CC BY-SA 2.0</small></small>
</div>
<p>
          The Altair was the first commercially successful desktop computer
          system in a box. The first microprocessor, the Intel 4004, came three
          years earlier, but during those three years anyone who wanted to use a
          microprocessor had to do a significant amount of engineering to
          combine the processor chip with memory and input/output circuitry. The
          Altair, with an eight-bit Intel 8080 processor chip, made it possible
          for hobbyists who were not electrical engineers to have a computer to
          program.
        </p>
<p>
          Prior to this time, computers were essentially huge, expensive devices
          owned only by universities, some large businesses, and a small number
          of other well-funded institutions. The Altair started a flood of
          inexpensive computers and computer kits. It was the beginning of the
          modern computer age.
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-apple-ii">
<div class="endnote">
<h4><strong>The Apple II</strong> (1977)</h4>
<div class="sidenoteBig">
<img alt="Apple II with TV and cassette storage" src="/bjc-r-ita/img/6-computers/apple-ii.png"/><br/><small><small>Apple II, Panasonic RQ-2102 cassette, and TV<br/>Photo credit:
              Carl Knoblock, Phil Pfeiffer
            </small></small>
</div>
<p>
          The Apple II, designed by Steve Wozniak using the eight-bit MOS
          Technology 6502 chip, was the first personal computer designed for the
          family market, rather than for intense hobbyists. Because it included
          a keyboard, it used an ordinary TV set as its monitor, and it could
          use an audio cassette tape recorder as its external memory (instead of
          more expensive disk drives), people could buy one, unpack the box,
          plug it in, and it would be working. It could display text and
          graphics in color, unlike earlier personal computer products. And it
          was inexpensive enough for parents to buy for their kids.
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-visicalc">
<div class="endnote">
<h4><strong>VisiCalc</strong> (1979)</h4>
<p>
          The first personal computer software intended specifically for
          business users was also the first interactive spreadsheet software on
          any computer. Business planning has always been done using
          paper-and-pencil spreadsheets, but the process of creating them was
          lengthy and error-prone. A program that could do the hard work and
          guarantee error-free results was eagerly welcomed by business.
        </p>
<p>
          A spreadsheet program is actually a special-purpose programming
          language, because users enter mathematical formulas (that is,
          functions, with numbers and <em>other cells</em> as inputs) into the
          cells of the spreadsheet. which then computes numeric values for all
          the cells when a number is entered into a starter cell.
        </p>
<p>
          VisiCalc sold hundreds of thousands of copies, and was responsible for
          much of the success of the Apple II, which businesses bought just as a
          platform for VisiCalc.
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-ibm-pc">
<div class="endnote">
<h4><strong>The IBM PC</strong> (1981)</h4>
<div class="sidenoteBig">
<img alt="original IBM PC" src="/bjc-r-ita/img/6-computers/ibm-pc.jpg"/><br/>
<small><small>
              Original IBM PC (Model 5150) with monochrome monitor<br/>
              By Ruben de Rijcke - Own work, CC BY-SA 3.0, via Wikimedia</small></small>
</div>
<p>
          Businesses were slow to adopt personal computers. Despite the
          popularity of VisiCalc, the eight-bit models seemed like toys and many
          businesses were reluctant to rely on them. This situation changed when
          IBM, the leader in large business computers, introduced their own
          personal computer, the 16-bit IBM PC. It was based on the Intel 8088,
          a variant of the 16-bit 8086 that used 16 bits inside the processor,
          but connected to memory and other components using an 8-bit bus, which
          made the other components slower (two bus operations needed per 16-bit
          word) but cheaper to build.
        </p>
<p>
          Because of their position as a near-monopoly manufacturer of large
          business computers, a personal computer from IBM was taken seriously
          and made businesses confident in buying PCs. VisiCalc quickly released
          a version of their spreadsheet software for the PC, although it was
          exactly compatible with the 8-bit version and so it didn't take full
          advantage of the 16-bit processor and greatly expanded memory of the
          PC.
        </p>
<p>
          Other companies started building 16-bit computers also, but at first
          they couldn't run software written for the IBM PC because IBM held the
          copyright on their operating system, PC-DOS. That OS was written for
          IBM by a young software company named Microsoft. They had a contract
          guaranteeing that Microsoft would not make PC-DOS available for
          non-IBM computers. This made it hard for competitors to get a start in
          the market.
        </p>
<p>
          But then Microsoft betrayed IBM by introducing a new, different
          operating system, MS-DOS, for PC-compatible computers. MS-DOS was
          virtually identical to PC-DOS, but with a few small differences so
          that it wasn't technically a violation of their contract with IBM. But
          MS-DOS made it possible for other computers to run software written
          for the PC. The other manufacturers couldn't use the name
          "PC," which was an IBM trademark, but users called them
          "PC-compatible." These competing models were often less
          expensive than the PC, so they were able to get customers despite
          IBM's reputation for building rugged, reliable equipment. The IBM PC
          remained popular with businesses, especially ones that also had large
          IBM "mainframe" computers, but they no longer had a monopoly
          on 16-bit computing. Rather, it was Microsoft that held the monopoly,
          on the operating system they all used.
        </p>
</div>
</div>
<div class="timeline collapse" id="timeline-mac">
<div class="endnote">
<h4><strong>The Apple Macintosh</strong> (1984)</h4>
<img alt="original Apple Macintosh" class="imageRight" src="/bjc-r-ita/img/6-computers/mac.jpg"/>
<p>
          When Steve Jobs visited Xerox PARC he was tremendously impressed by
          the user interface of the Alto. He knew that a computer like that
          could be a huge seller, especially with people who were repelled by
          the PC-DOS user experience. He hired away some of the PARC team to
          build a computer for Apple that would be similar to the Alto.
        </p>
<p>
          Apple's first try, the Lisa, was not a success; it was too expensive,
          and a little too much before its time. But then came the Macintosh. It
          was introduced to the world by an innovative advertisement on the 1984
          Super Bowl broadcast, in which the PC was associated with the
          dictatorial society of George Orwell's novel <em>1984, </em>and the
          Mac freed the society. (You can find the ad on YouTube.) They also
          used the slogan "the computer for the rest of us," which is
          still widely quoted. (People have, for example, described CS
          Principles as the computer science course for the rest of us.)
        </p>
<p>
          Since Microsoft quickly switched from PC-DOS to the Windows operating
          system that copied the Macintosh, you have grown up in a world in
          which the "WIMP" (windows, icons, menus, pointers) user
          interface is universally used on computers. So it may be hard for you
          to realize what a revolutionary change the Macintosh made. It was the
          first computer system you could just sit down and start using without
          reading the manual.
        </p>
</div>
</div>
<br clear="all"/>

<div class="forYouToDo" id="first">
<ol>
<li>
          Discussion:
          <ul>
<li>Does a device have to be programmable to be a computer?</li>
<li>Does it have to operate by itself?</li>
</ul>
</li>
</ol>
</div>
<div class="takeNote">
      Here are two key ideas:
      <ul>
<li>
<em>Software,</em> in the form of a program stored in the computer's
          memory, is, itself, a kind of abstraction. It is what makes a computer
          usable for more than one purpose.<br/>
</li>
<li>
          We didn't get <em>usable</em> computers until there was an underlying
          technology (the transistor) small enough, inexpensive enough, and fast
          enough to support the program abstraction.
        </li>
</ul>
</div>
</body>
</html>
